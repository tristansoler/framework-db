{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc0212c-3e00-4db6-adf7-df9bd209a479",
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ExpiredToken) when calling the GetObject operation: The provided token has expired.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--dataflow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample-flow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--source-file-path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdsds/dsdas\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--bucket-prefix\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdsdasds\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m ]\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mConfigSetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\x850374\\Projects\\platform\\data_framework\\src\\modules\\config\\core.py:54\u001b[0m, in \u001b[0;36mConfigSetup.__init__\u001b[1;34m(self, parameters)\u001b[0m\n\u001b[0;32m     51\u001b[0m dataflow \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataflow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m bucket_prefix \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbucket_prefix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m json_config \u001b[38;5;241m=\u001b[39m \u001b[43mConfigSetup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_config_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucket_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_instancia\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m ConfigSetup\u001b[38;5;241m.\u001b[39mparse_to_model(model\u001b[38;5;241m=\u001b[39mConfig, parameters\u001b[38;5;241m=\u001b[39mparameters, json_file\u001b[38;5;241m=\u001b[39mjson_config)\n",
      "File \u001b[1;32mc:\\Users\\x850374\\Projects\\platform\\data_framework\\src\\modules\\config\\core.py:82\u001b[0m, in \u001b[0;36mConfigSetup.read_config_file\u001b[1;34m(cls, dataflow, bucket_prefix, is_local)\u001b[0m\n\u001b[0;32m     79\u001b[0m     bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbucket_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_code\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     80\u001b[0m     key_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataflow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/config/transformations.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(json\u001b[38;5;241m.\u001b[39mloads(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()))\n\u001b[0;32m     86\u001b[0m common_flow_json \u001b[38;5;241m=\u001b[39m current_flow_json \u001b[38;5;241m=\u001b[39m config_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\botocore\\client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\site-packages\\botocore\\client.py:1017\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (ExpiredToken) when calling the GetObject operation: The provided token has expired."
     ]
    }
   ],
   "source": [
    "from data_framework.modules.utils.logger import logger\n",
    "from data_framework.modules.storage.core_storage import Storage\n",
    "from data_framework.modules.config.core import ConfigSetup\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    \"sample.py\",\n",
    "    '--dataflow', 'sample-flow',\n",
    "    '--source-file-path', 'dsds/dsdas',\n",
    "    '--bucket-prefix', 'dsdasds'\n",
    "]\n",
    "\n",
    "print(ConfigSetup())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8274426e-c212-4076-8f08-a580945b8558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'processes': {'landing_to_raw': {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}}, 'environment': 'develop'}\n",
      "model -> <class 'data_framework.modules.config.model.flows.Config'> | json_file -> {'processes': {'landing_to_raw': {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}}, 'environment': 'develop'}\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.Processes'> field -> processes\n",
      "model -> <class 'data_framework.modules.config.model.flows.Processes'> | json_file -> {'landing_to_raw': {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}}\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.LandingToRaw'> field -> landing_to_raw\n",
      "model -> <class 'data_framework.modules.config.model.flows.LandingToRaw'> | json_file -> {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.IncomingFileLandingToRaw'> field -> incoming_file\n",
      "model -> <class 'data_framework.modules.config.model.flows.IncomingFileLandingToRaw'> | json_file -> {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}\n",
      "[else] field_type -> typing.Optional[str] field -> zipped\n",
      "[else] field_type -> <enum 'LandingFileFormat'> field -> file_format\n",
      "[else] field_type -> <class 'str'> field -> filename_pattern\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.CSVSpecs'> field -> csv_specs\n",
      "model -> <class 'data_framework.modules.config.model.flows.CSVSpecs'> | json_file -> {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}\n",
      "[else] field_type -> <class 'int'> field -> header_position\n",
      "[else] field_type -> <class 'bool'> field -> header\n",
      "[else] field_type -> <class 'str'> field -> encoding\n",
      "[else] field_type -> <class 'str'> field -> delimiter\n",
      "[else] field_type -> <enum 'DateLocated'> field -> date_located\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.DateLocatedFilename'> field -> date_located_filename\n",
      "model -> <class 'data_framework.modules.config.model.flows.DateLocatedFilename'> | json_file -> {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}\n",
      "[else] field_type -> <class 'str'> field -> regex\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.Validations'> field -> validations\n",
      "model -> <class 'data_framework.modules.config.model.flows.Validations'> | json_file -> {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}\n",
      "[else] field_type -> <class 'bool'> field -> validate_extension\n",
      "[else] field_type -> <class 'bool'> field -> validate_filename\n",
      "[else] field_type -> <class 'bool'> field -> validate_csv\n",
      "[else] field_type -> <class 'bool'> field -> validate_columns\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.DatabaseTable'> field -> output_file\n",
      "model -> <class 'data_framework.modules.config.model.flows.DatabaseTable'> | json_file -> {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}\n",
      "[else] field_type -> <enum 'Database'> field -> database\n",
      "[else] field_type -> <class 'str'> field -> database_relation\n",
      "[else] field_type -> <class 'str'> field -> table\n",
      "[else] field_type -> typing.Optional[str] field -> partition_field\n",
      "[else] field_type -> typing.Optional[list] field -> primary_keys\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.ProcessingSpecifications'> field -> processing_specifications\n",
      "model -> <class 'data_framework.modules.config.model.flows.ProcessingSpecifications'> | json_file -> {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}\n",
      "[else] field_type -> <enum 'Technologies'> field -> technology\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.Hardware'> field -> hardware\n",
      "model -> <class 'data_framework.modules.config.model.flows.Hardware'> | json_file -> {'ram': 512}\n",
      "[else] field_type -> <class 'int'> field -> ram\n",
      "[else] field_type -> typing.Optional[int] field -> cpu\n",
      "[else] field_type -> typing.Optional[int] field -> disk\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.SparkConfiguration'> field -> spark_configuration\n",
      "model -> <class 'data_framework.modules.config.model.flows.SparkConfiguration'> | json_file -> {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}\n",
      "[else] field_type -> <class 'bool'> field -> default_catalog\n",
      "[else] field_type -> <enum 'Database'> field -> warehouse\n",
      "[else] field_type -> typing.List[data_framework.modules.config.model.flows.CustomConfiguration] field -> custom_configuration\n",
      "[else] field_type -> typing.Optional[data_framework.modules.config.model.flows.RawToStaging] field -> raw_to_staging\n",
      "[else] field_type -> typing.Optional[data_framework.modules.config.model.flows.ToOutput] field -> to_output\n",
      "[else] field_type -> <enum 'Environment'> field -> environment\n",
      "[elif] field_type -> <class 'data_framework.modules.config.model.flows.Parameters'> field -> parameters\n",
      "model -> <class 'data_framework.modules.config.model.flows.Parameters'> | json_file -> {'environment': 'local', 'dataflow': 'morningstar', 'process': 'landing_to_raw', 'flow': 'dividend_prod', 'source_file_path': 'dsds/dsdas', 'bucket_prefix': 'dsdasds', 'region': 'eu-west-1'}\n",
      "[else] field_type -> <class 'str'> field -> dataflow\n",
      "[else] field_type -> <class 'str'> field -> process\n",
      "[else] field_type -> <class 'str'> field -> table\n",
      "[else] field_type -> <class 'str'> field -> source_file_path\n",
      "[else] field_type -> <class 'str'> field -> bucket_prefix\n",
      "[else] field_type -> <class 'str'> field -> file_name\n",
      "[else] field_type -> typing.Optional[str] field -> file_date\n",
      "[else] field_type -> <class 'str'> field -> region\n",
      "{'processes': {'landing_to_raw': {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}}, 'environment': 'develop'}\n",
      "model -> <class 'data_framework.modules.config.model.flows.Config'> | json_file -> {'processes': {'landing_to_raw': {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}}, 'environment': 'develop'}\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.Processes'> field -> processes\n",
      "model -> <class 'data_framework.modules.config.model.flows.Processes'> | json_file -> {'landing_to_raw': {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}}\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.LandingToRaw'> field -> landing_to_raw\n",
      "model -> <class 'data_framework.modules.config.model.flows.LandingToRaw'> | json_file -> {'incoming_file': {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}, 'output_file': {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}, 'processing_specifications': {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}}\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.IncomingFileLandingToRaw'> field -> incoming_file\n",
      "model -> <class 'data_framework.modules.config.model.flows.IncomingFileLandingToRaw'> | json_file -> {'zipped': 'zip', 'file_format': 'csv', 'filename_pattern': 'sample_(\\\\d{4})_(\\\\d{2})_(\\\\d{2})', 'csv_specs': {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}, 'validations': {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}}\n",
      "[else] field_type -> typing.Optional[str] field -> zipped\n",
      "[else] field_type -> <enum 'LandingFileFormat'> field -> file_format\n",
      "[else] field_type -> <class 'str'> field -> filename_pattern\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.CSVSpecs'> field -> csv_specs\n",
      "model -> <class 'data_framework.modules.config.model.flows.CSVSpecs'> | json_file -> {'header_position': 0, 'header': True, 'encoding': 'UTF-8', 'delimiter': ';', 'date_located': 'filename', 'date_located_filename': {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}}\n",
      "[else] field_type -> <class 'int'> field -> header_position\n",
      "[else] field_type -> <class 'bool'> field -> header\n",
      "[else] field_type -> <class 'str'> field -> encoding\n",
      "[else] field_type -> <class 'str'> field -> delimiter\n",
      "[else] field_type -> <enum 'DateLocated'> field -> date_located\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.DateLocatedFilename'> field -> date_located_filename\n",
      "model -> <class 'data_framework.modules.config.model.flows.DateLocatedFilename'> | json_file -> {'regex': '(\\\\d{4})(\\\\d{2})(\\\\d{2})'}\n",
      "[else] field_type -> <class 'str'> field -> regex\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.Validations'> field -> validations\n",
      "model -> <class 'data_framework.modules.config.model.flows.Validations'> | json_file -> {'validate_extension': True, 'validate_filename': True, 'validate_csv': True, 'validate_columns': True}\n",
      "[else] field_type -> <class 'bool'> field -> validate_extension\n",
      "[else] field_type -> <class 'bool'> field -> validate_filename\n",
      "[else] field_type -> <class 'bool'> field -> validate_csv\n",
      "[else] field_type -> <class 'bool'> field -> validate_columns\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.DatabaseTable'> field -> output_file\n",
      "model -> <class 'data_framework.modules.config.model.flows.DatabaseTable'> | json_file -> {'database': 'funds_raw', 'table': 'product_test', 'partitions': {'datadate': True, 'insert_time': False}}\n",
      "[else] field_type -> <enum 'Database'> field -> database\n",
      "[else] field_type -> <class 'str'> field -> database_relation\n",
      "[else] field_type -> <class 'str'> field -> table\n",
      "[else] field_type -> typing.Optional[str] field -> partition_field\n",
      "[else] field_type -> typing.Optional[list] field -> primary_keys\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.ProcessingSpecifications'> field -> processing_specifications\n",
      "model -> <class 'data_framework.modules.config.model.flows.ProcessingSpecifications'> | json_file -> {'technology': 'emr', 'hardware': {'ram': 512}, 'spark_configuration': {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}}\n",
      "[else] field_type -> <enum 'Technologies'> field -> technology\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.Hardware'> field -> hardware\n",
      "model -> <class 'data_framework.modules.config.model.flows.Hardware'> | json_file -> {'ram': 512}\n",
      "[else] field_type -> <class 'int'> field -> ram\n",
      "[else] field_type -> typing.Optional[int] field -> cpu\n",
      "[else] field_type -> typing.Optional[int] field -> disk\n",
      "[if] field_type -> <class 'data_framework.modules.config.model.flows.SparkConfiguration'> field -> spark_configuration\n",
      "model -> <class 'data_framework.modules.config.model.flows.SparkConfiguration'> | json_file -> {'default_catalog': True, 'warehouse': 'funds_raw', 'custom_configuration': [{'parameter': '', 'value': ''}]}\n",
      "[else] field_type -> <class 'bool'> field -> default_catalog\n",
      "[else] field_type -> <enum 'Database'> field -> warehouse\n",
      "[else] field_type -> typing.List[data_framework.modules.config.model.flows.CustomConfiguration] field -> custom_configuration\n",
      "[else] field_type -> typing.Optional[data_framework.modules.config.model.flows.RawToStaging] field -> raw_to_staging\n",
      "[else] field_type -> typing.Optional[data_framework.modules.config.model.flows.ToOutput] field -> to_output\n",
      "[else] field_type -> <enum 'Environment'> field -> environment\n",
      "[elif] field_type -> <class 'data_framework.modules.config.model.flows.Parameters'> field -> parameters\n",
      "model -> <class 'data_framework.modules.config.model.flows.Parameters'> | json_file -> {'environment': 'local', 'dataflow': 'morningstar', 'process': 'landing_to_raw', 'flow': 'dividend_prod', 'source_file_path': 'dsds/dsdas', 'bucket_prefix': 'dsdasds', 'region': 'eu-west-1'}\n",
      "[else] field_type -> <class 'str'> field -> dataflow\n",
      "[else] field_type -> <class 'str'> field -> process\n",
      "[else] field_type -> <class 'str'> field -> table\n",
      "[else] field_type -> <class 'str'> field -> source_file_path\n",
      "[else] field_type -> <class 'str'> field -> bucket_prefix\n",
      "[else] field_type -> <class 'str'> field -> file_name\n",
      "[else] field_type -> typing.Optional[str] field -> file_date\n",
      "[else] field_type -> <class 'str'> field -> region\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameters(dataflow='morningstar', process='landing_to_raw', table=None, source_file_path='dsds/dsdas', bucket_prefix='dsdasds', file_name=None, file_date=None, region='eu-west-1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from data_framework.modules.config.core import config, ConfigSetup\n",
    "from data_framework.modules.catalogue.core_catalogue import CoreCatalogue\n",
    "\n",
    "os.environ[\"ENV\"] = \"local\"\n",
    "\n",
    "sys.argv = [\n",
    "    \"sample.py\",\n",
    "    '--environment', 'local',\n",
    "    '--dataflow', 'morningstar',\n",
    "    '--process', 'landing_to_raw',\n",
    "    '--flow', 'dividend_prod',\n",
    "    '--source-file-path', 'dsds/dsdas',\n",
    "    '--bucket-prefix', 'dsdasds',\n",
    "    #'--file-name', 'sample.zip',\n",
    "    #'--file-date', '2024-10-09',\n",
    "    '--region', 'eu-west-1'\n",
    "]\n",
    "\n",
    "ConfigSetup()\n",
    "\n",
    "#from flows.landing import ProcessingCoordinator\n",
    "\n",
    "#coordinator = ProcessingCoordinator()\n",
    "#coordinator.process()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65ca28a3-ce12-46e0-b9e0-9f7c434f654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e5fef-c19e-4ccb-8810-0f3e105349ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
